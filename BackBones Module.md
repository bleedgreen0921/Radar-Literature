

# BackBones Module

## LeNet

LeNet算法作为卷积神经网络（CNN）的奠基性工作，提出了多个关键概念，深刻影响了后续深度学习的发展。

### **局部感受野（Local Receptive Fields）**

卷积核仅覆盖输入图像的局部区域（如5×5像素），而非全连接，模拟生物视觉皮层的局部感知机制。

通过局部特征提取（如边缘、角点）降低计算复杂度

增强模型对图像局部模式的捕捉能力，避免全局冗余信息干扰

### **权值共享（Weight Sharing）**

同一卷积核在整张图像上滑动时复用权重参数。

例如LeNet-5的C1层仅有6个卷积核（每核25参数），相比全连接层参数量减少约99%

同一特征在不同位置由相同核检测，提升对目标位置变化的鲁棒性。

### **池化层（Pooling Layers）与下采样**

通过空间下采样（如2×2平均池化）压缩特征图尺寸，逐步抽象高层语义特征。

### **层次化网络结构**

交替堆叠卷积层（特征提取）与池化层（特征压缩），形成“低层边缘→中层部件→高层语义”的层次化特征表达。

### **激活函数与非线性建模**

使用Sigmoid或双曲正切（Tanh）函数引入非线性，突破线性模型的表达能力限制。

### **端到端训练**

直接从原始像素输入到分类结果输出，无需手工设计特征。

### **反向传播**

反向传播算法优化网络参数，验证了深度网络的可训练性，参数初始化方法（如随机初始化）与梯度下降策略为后续模型奠定基础。

反向传播（**Backpropagation**，简称BP）是神经网络训练的核心算法，通过**梯度下降法**优化网络参数，使其输出逼近目标值。

#### 基本原理

反向传播基于**链式法则**（Chain Rule）实现误差梯度的高效计算，其核心流程分为两个阶段：

1. **正向传播**（Forward Propagation）

   输入数据逐层传递，经加权求和（线性变换）和激活函数（非线性变换）处理后生成预测输出。

2. **反向传播**（Backward Propagation）

   从输出层开始，逐层计算损失函数对权重和偏置的梯度，并通过梯度下降更新参数。

#### 数学推导与梯度计算

- **链式法则**

以两层神经网络为例，损失函数L对权重Wij的梯度为：



## Alexnet

​	现在，我们已经准备好描述CNN的整体架构。如图2所示，网包含八层带重量的层；前五个是卷积的，其余三个是完全连接的。最后一个完全连接层的输出被馈送到1000路softmax，该softmax在1000个类标签上产生分布。我们的网络最大化多项式逻辑回归目标，这相当于最大化预测分布下正确标签的对数概率在训练案例中的平均值。

​	第二、第四和第五卷积层的内核仅连接到位于同一GPU上的前一层中的内核映射（见图2）。第三卷积层的核连接到第二层中的所有核映射。全连接层中的神经元与前一层中的所有神经元相连。响应归一化层位于第一和第二卷积层之后。第3.4节中描述的最大池化层遵循响应归一化层和第五卷积层。ReLU非线性被应用于每个卷积层和全连接层的输出。

​	第一卷积层用96个大小为11×11×3的核对224×224×3的输入图像进行滤波，步长为4像素（这是相邻接收野中心之间的距离核图中的神经元）。第二个卷积层将第一个卷积层的（响应归一化和合并）输出作为输入，并用256个大小为5×5×48的核对其进行滤波。

​	第三、第四和第五卷积层彼此连接，没有任何中间的池化或归一化层。第三卷积层有384个大小为3×3×256的核，连接到第二卷积层的（归一化、合并）输出。第四卷积层有384个大小为3×3×192的核，第五卷积层有256个大小为2×3×193的核。每个完全连接的层有4096个神经元。![image-20250428153225885](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250428153225885.png)

### 卷积输出计算公式：![image-20250428153355339](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250428153355339.png)

最大池化输出计算公式同上，只不过核的尺寸改为池化窗口大小。

**Conv1**

- 输入：224 × 224 × 3
- 卷积核：11 × 11
- 数量：96
- 步长：4
- 填充：**2**

输出尺寸：55×55×96

------

**Norm1**

- 局部响应归一化（LRN），
- **不改变尺寸**。

输出还是：55×55×96

------

**Pool1**

- 最大池化：3×3窗口
- 步长：2
- 填充：默认 0（无填充）

计算输出尺寸：27×27×96

**Conv2**

输入：

- 27×27×96
- 卷积核：5×5
- 数量：256
- 步长：1
- 填充：2

输出尺寸：27×27×256

------

**Norm2**

局部响应归一化（LRN），**不改变尺寸**：27×27×256

------

**Pool2**

最大池化（3×3窗口，步长=2）：

输出尺寸：

13×13×256

------

**Conv3**

输入：

- 13×13×256
- 卷积核：3×3
- 数量：384
- 步长：1
- 填充：1

输出尺寸：

13×13×384

------

**Conv4**

输入：

- 13×13×384
- 卷积核：3×33
- 数量：384
- 步长：1
- 填充：1

输出尺寸：

13×13×384

------

**Conv5**

输入：

- 13×13×384
- 卷积核：3×3
- 数量：256
- 步长：1
- 填充：1

输出尺寸：

13×13×256

------

**Pool5**

最大池化（3×3窗口，步长=2）：

输出尺寸：

6×6×256

**全连接部分**

| 层   | 操作   | 细节                                 | 输出 |
| :--- | :----- | :----------------------------------- | :--- |
| FC6  | 全连接 | 输入6×6×256（9216），输出4096        | 4096 |
| FC7  | 全连接 | 输入4096，输出4096                   | 4096 |
| FC8  | 全连接 | 输入4096，输出1000类（ImageNet分类） | 1000 |

### **DropOut**

**Dropout** 是一种**正则化（Regularization）技术**，主要用于**防止神经网络过拟合**。

它的核心思想非常简单：

- 在每次前向传播时，
- 以一定概率 p （比如 0.5），**随机地"丢弃"（屏蔽）一些神经元**，
- 丢弃的神经元不会参与后续的计算（既不会传递激活值，也不会参与反向传播的权重更新）。

这样，网络在训练过程中不会依赖于某些特定神经元，提升模型的**泛化能力**。

训练时：

- 对每个神经元，随机生成一个0或1。
- 1表示保留（按比例缩放），0表示丢弃。
- 只有被保留的神经元才参与计算。

推理（测试）时：

- **不开启Dropout**，而是把所有神经元都使用，
- 同时把它们的输出乘以保留概率 ppp 来保证均值一致。

DropOut的优点：

- 避免模型**过拟合**。

- **增强模型鲁棒性**：即使丢掉一部分神经元，模型整体功能也不被破坏。

- 相当于在训练过程中**集成（Ensemble）了大量子网络**，效果非常好。

总结：**Dropout** = 训练时随机屏蔽部分神经元，以提升神经网络的泛化性能，防止过拟合。

### ReLU激活函数

​	ReLU的全称是**Rectified Linear Unit**，即**修正线性单元**。AlexNet的**所有卷积层和全连接层**均采用ReLU激活函数，替代了传统的Sigmoid和Tanh。

**数学表达式：**
$$
ReLU(x)=max(0,x)
$$
**优势与局限性：**

- **计算高效性**

  ReLU的梯度计算仅需判断输入正负，正向传播和反向传播的复杂度远低于Sigmoid，显著加速了训练过程。实验表明，ReLU使AlexNet的训练速度比使用Sigmoid快**6倍**以上。

- **缓解梯度消失问题**

  在正区间（*x*>0），ReLU的导数为1，梯度恒定，避免了深层网络中梯度因连续相乘而指数级衰减的问题。

- **稀疏激活与泛化能力**

  ReLU将负值置零，使得网络中约50%的神经元在训练过程中处于非激活状态，形成**稀疏激活**模式，减少了参数间的冗余依赖，增强模型泛化能力。

- **与局部响应归一化（LRN）的协同**

  AlexNet在ReLU后引入LRN层，通过归一化局部神经元的响应值，抑制过强激活，进一步提升特征的区分度。

- **“死亡ReLU”问题**

  当输入持续为负时，神经元输出恒为0，梯度无法更新，导致神经元“死亡”。

  **解决方法：**

  - **数据增强**：通过随机裁剪、镜像翻转增加输入多样性，减少负值输入的累积。
  - **参数初始化**：采用He初始化（未在原文明确提及，但后续研究证实其有效性），调整权重分布以避免初始阶段大量负激活。

## VGG

​	研究了卷积网络**深度**对其在大规模图像识别设置中的准确性的影响。使用具有**非常小（3*3）卷积滤波器**的架构对深度增加的网络进行了全面评估，表明通过将深度提升到16-19个权重层，可以实现对现有技术配置的显著提升。

### 核心思想

- **小卷积核堆叠取代大卷积核**

  全网络使用**3×3小卷积核**替代AlexNet中的大卷积核（如11×11、5×5），通过堆叠多层小核实现与大核等效的感受野。两个3×3卷积层的感受野等效于一个5×5卷积层，但参数减少28%。

  每层卷积后接ReLU激活，堆叠多层后比单层大核多出更多非线性变换，提升特征表达能力。

  小核更擅长提取局部细节（如边缘、纹理），为高层语义特征提供更丰富的底层信息。

- **网络深度和结构一致性**

  首次系统探索网络深度对性能的影响，提出**VGG16（16层）**和**VGG19（19层）**，证明深度增加可显著降低分类错误率。

  所有卷积层采用**固定步长1**和**相同边缘填充（padding=1）**，保持特征图空间分辨率。

  池化层统一使用**2×2最大池化（stride=2）**，逐步压缩特征图尺寸。

- **全连接层转卷积层（测试阶段）**

  在测试阶段将训练时的全连接层替换为卷积层（如7×7卷积替代第一全连接层），使网络可处理任意尺寸输入。

- **激活函数优化与LRN去除**

  全网络采用ReLU激活，缓解深层网络的梯度消失问题，相比AlexNet的Tanh训练速度提升约40%。

  弃用局部响应归一化（LRN）：实验证明LRN对性能提升有限，去除后简化网络结构并降低计算开销

- **模块化设计与迁移学习优势**

  通过重复**Conv-Block（卷积层组）**构建网络，每组包含2-3个卷积层+池化层，便于扩展和调整深度。

  VGG的预训练模型（如VGG16）在ImageNet上提取的通用特征，成为目标检测（Faster R-CNN）、语义分割（FCN）等任务的基准特征提取器。

### 网络结构

**输入：**

​	卷积网络的输入是一个**固定大小的224×224** RGB图像。所做的唯一预处理是从每个像素中减去在训练集上计算的平均RGB值。

**卷积层：**

​	使用具有**非常小的感受野**的滤波器：**3×3**（这是捕捉左/右、上/下、中心概念的最小尺寸）。

​	**卷积步长固定为1像素**。

​	为使卷积后保持空间分辨率，3×3 conv层的**填充为1像素**。

**池化层：**

​	最大池化是在2×2像素的窗口上执行的，步幅为2。

**全连接层：**

​	前两个层各有4096个通道，第三个层执行1000路ILSVRC分类，因此包含1000个通道（每个类别一个）。最后一层是 soft-max 层。全连接层的配置在所有网络中都是相同的。

## ResNet（残差网络）

![image-20250507163847042](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250507163847042.png)

### 残差学习（Residual Learning）

​	传统深度神经网络随着层数增加，训练集和测试集准确率可能不升反降，这种现象称为**“退化问题”（Degradation Problem）**。ResNet通过**残差学习**（Residual Learning）解决了这一问题。其核心公式为：
$$
H(x)=F(x)+x
$$
其中，*F*(*x*) 表示网络需要学习的残差函数（输入与输出的差异），而**跳跃连接（Skip Connection）**直接将输入 *x* 传递到输出端。这种设计让网络无需直接学习复杂的映射，而是专注于学习差异部分，简化了优化过程。

### 跳跃连接（Skip Connection）

​	跳跃连接允许梯度在反向传播时绕过部分层直接传递到浅层，缓解了梯度消失或爆炸问题。即使深层网络参数更新困难，梯度仍可通过残差路径稳定传递。

​	通过跳跃连接强制网络保留原始输入信息，即使某些层未学到有效特征，输入仍能无损传递至后续层，避免信息丢失。

### 残差块（Residual Block）的设计

**BasicBlock**：用于浅层网络（如ResNet-18/34），由两个3×3卷积层构成，通过跳跃连接直接相加输入与输出

**Bottleneck Block**：用于深层网络（如ResNet-50/101/152），采用1×1→3×3→1×1的卷积组合，通过降维减少计算量（如将256通道压缩为64通道后再恢复），提升效率。

### 分阶段的网络结构

​	ResNet将网络分为四个阶段，每个阶段通过步长为2的卷积降采样特征图尺寸，同时通道数翻倍（如64→128→256→512）。每个阶段堆叠多个残差块，逐步提取高阶特征。

### 全局平均池化代替全连接层

​	传统CNN使用全连接层进行分类，而ResNet以全局平均池化（Global Average Pooling）替代，将特征图直接压缩为向量，减少参数量的同时防止过拟合。

### 模块化设计与参数效率

​	残差块可灵活堆叠，支持快速扩展网络深度。尽管网络层数大幅增加，但因Bottleneck结构和全局池化，参数量并未显著上升

## DenseNet（密集卷积网络）

### 密集连接（Dense Connectivity）

DenseNet的核心设计是让网络中的每一层都直接连接到后续所有层。具体来说，在第 *l* 层中，其输入是前 *l*−1 层所有输出的拼接（Concatenation），公式为：
$$
x_l=H_l([x_0,x_1,…,x_{l-1}])
$$
这种机制打破了传统神经网络的线性连接方式，使得每一层都能直接访问所有前层特征图，从而**保留原始信息并增强特征复用**

### 特征复用（Feature Reuse）

通过跨层拼接，DenseNet允许后续层直接利用前层提取的低级特征（如边缘、纹理）和高级语义特征，避免了传统网络中低级特征可能被高级特征覆盖的问题。这种机制显著提升了特征利用率，尤其在数据量较少时表现更优。

**缓解梯度消失问题**：

密集连接的短路径梯度传播特性，使得反向传播时梯度可直接传递到浅层网络，解决了深层网络中的梯度消失问题。

### 模块化结构设计

**DenseBlock**：

​	每个DenseBlock内部通过密集连接堆叠多层，所有层共享相同的特征图尺寸。每层包含BN-ReLU-Conv操作，通常采用Bottleneck结构（1×1卷积降维→3×3卷积→特征拼接）减少计算量。

**过渡层（Transition Layer）**：

​	用于连接不同DenseBlock，包含1×1卷积压缩通道数和2×2平均池化降采样。通过压缩因子（如0.5）进一步减少参数，提升模型效率。

**Growth Rate（增长率）**：

​	定义每个DenseBlock内每层新增的特征图通道数（如k=32）。通过控制k值，平衡模型复杂度和性能。

**特征拼接替代加法**：

​	相较于ResNet的残差相加，DenseNet的通道拼接保留了更多原始特征，减少了信息损失，同时避免了特征融合时的维度冲突。

**全局特征压缩与分类优化**：

​	在网络的末端，DenseNet采用**全局平均池化（Global Average Pooling）**替代传统全连接层，将特征图直接压缩为分类向量，减少参数量并降低过拟合风险。

## MobileNetV2

​	MobileNetV2是一种专门为移动和资源受限环境量身定制的新的神经网络架构，通过显著减少所需的操作和内存数量，同时保持相同的准确性，推动了移动定制计算机视觉模型的最新发展。

​	主要贡献是一个新的层模块：具有线性瓶颈的反向残差。该模块将低位压缩表示作为输入，首先将其扩展至高维，并用轻量级深度可分离卷积进行滤波，特征随后通过线性卷积投影回低维表示。它可以通过从不完全实现大型中间张量来显著减少推理过程中所需的内存占用。

​	MobileNetV2引入了线性瓶颈和反向残差结构，以便通过利用问题的低秩特性来制作更高效的层结构。当且仅当输入和输出具有相同数量的通道时，它们才通过残差连接连接。这种结构在输入和输出处保持紧凑的表示，同时在内部扩展到更高维的特征空间，以提高非线性全通道变换的表现力。	

### 反向残差（Inverted Residuals）

传统 ResNet 的残差结构是这样的：Input → [降维] → [3x3卷积] → [升维] → 加上原始输入

MobileNetV2 的反向残差是：Input → [升维] → [depthwise卷积] → [降维] → 残差连接

也就是说：**输入通道先扩展（变“宽”），中间做空间操作，最后再压缩回原通道数**，因此叫做“**反向残差结构（Inverted Residual）**”。

MobileNetV2 是轻量网络，它采用了深度可分离卷积（depthwise separable conv）来减小计算量。

但深度卷积本身**不具备跨通道的信息融合能力**，所以：

- **先升维**，让后续操作有更多通道可用（增加表达能力）
- **再降维**，降低计算与参数开销
- **残差连接仅用于输入输出通道相同、stride=1 的 block**

### 线性瓶颈（Linear Bottlenecks）

**传统设计：非线性激活（ReLU）用于每个卷积后，增加模型非线性表达能力。**

但是！在 MobileNetV2 的实验中发现：

**ReLU 在低维空间中容易导致信息丢失，特别是投影降维阶段。**

 所以：MobileNetV2 的解决方案是：

- **在瓶颈 block 的最后一层（1×1 降维卷积）之后，不再使用 ReLU**，而是使用**线性激活（即不使用激活函数）**
- 这就称为“**线性瓶颈（Linear Bottleneck）**”

**为什么有效？**

- 深层网络中的 ReLU 可能会**把低维特征“压塌”（激活为 0）**

- 而使用线性激活可以**更好保留底层特征信息**

- 搭配反向残差连接时，能更稳定地传递信息（尤其是输入和输出通道数相同时）

| 概念          | MobileNetV1 / ResNet | MobileNetV2                             |
| ------------- | -------------------- | --------------------------------------- |
| 残差结构方向  | 正向（降维→升维）    | 反向（升维→降维）                       |
| 卷积类型      | 标准卷积             | 深度可分离卷积（depthwise + pointwise） |
| 是否使用 ReLU | 所有卷积后都用       | 降维卷积之后不使用 ReLU（线性瓶颈）     |
| 目的          | 提升非线性表达       | 保留特征信息，减少信息压缩损失          |

### 深度可分离卷积（Depthwise Separable Convolutions）

​	深度可分离卷积（**Depthwise Separable Convolution**）是轻量级神经网络中常用的一种高效卷积操作，它通过**分解标准卷积操作**，显著减少了模型的计算量和参数量，同时仍保持较强的特征提取能力。基本思想是用一个分解的版本替换一个完整的卷积算子，将卷积分为两个单独的层。第一层称为深度卷积，它通过在每个输入通道应用单个卷积滤波器来执行轻量级滤波。第二层是1×1卷积，称为逐点卷积，负责通过计算输入通道的线性组合来构建新特征。

- **Depthwise 卷积（按通道分组）**

  每个输入通道用一个独立的 `K×K` 卷积核处理，不做通道间融合。输出仍为 `C_in` 个通道。

  计算量为：
  $$
  K⋅K⋅C_{in}⋅H⋅W
  $$

- **Pointwise 卷积（1×1 卷积）**

  使用 `C_out` 个 `1×1` 卷积核对 `C_in` 个通道进行融合（线性组合）输出通道数变为 `C_out`

  计算量为：
  $$
  C_{in}⋅C_{out}⋅H⋅W
  $$

  | 优点                 | 解释                                                  |
  | -------------------- | ----------------------------------------------------- |
  | **计算效率高**       | 大大减少 FLOPs 与参数量，特别适合移动设备和嵌入式系统 |
  | **可扩展性强**       | 可配合 Bottleneck、残差结构做更深网络                 |
  | **效果好**           | 在精度损失很小的前提下，换取大量的推理速度提升        |
  | **与其它技术兼容好** | 可结合注意力、残差、量化等模块构建高性能轻量模型      |

## MobileNetV3

**互补搜索技术（complementary search techniques）**

用于分块搜索的平台感知NAS

Net适配分层搜索

适用于移动环境的新型高效非线性

新型高效网络设计

新型高效分段解码器

------

## Transformer

**✅ 简要介绍：什么是 Transformer？**

**Transformer** 是一种基于 **自注意力机制（Self-Attention）** 的神经网络架构，最早由 Google 于 2017 年在论文《Attention is All You Need》中提出，用于解决自然语言处理（NLP）中的序列建模问题，如机器翻译、文本生成等。

------

**🔑 核心思想：Self-Attention + 并行计算**

Transformer 的核心思想包括以下几点：

1. **自注意力机制（Self-Attention）**

- 每个输入元素（如一个词或向量）可以对序列中的其他元素动态“关注”。
- 通过计算 Query、Key、Value 三组向量，使模型能捕捉长距离依赖关系。

2. **并行计算**

- 不再依赖 RNN 或 LSTM 的时间步结构，可直接对整个序列进行 **并行处理**，显著提升训练效率。

3. **位置编码（Positional Encoding）**

- 因为 Transformer 不处理序列顺序，所以需要**显式添加位置编码**表示每个词在序列中的位置。

------

**🧠 Transformer 架构简图**

通常由 **Encoder** 和 **Decoder** 两部分组成（如原始论文结构）：

- **Encoder**：输入文本 → 多层自注意力 + 前馈网络
- **Decoder**：生成输出文本 → 自注意力 + 编码器-解码器注意力 + 前馈网络

而在 **BERT、GPT** 等模型中，往往只保留 Encoder 或 Decoder。

------

### Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

------

移位窗口方案通过将自注意力计算限制在非重叠的局部窗口，同时允许跨窗口连接，从而提高了效率。这种分层架构具有在各种尺度上建模的灵活性，并且相对于图像大小具有线性计算复杂性。

<img src="C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250604160052934.png" alt="image-20250604160052934" style="zoom: 67%;" />

- Swin Transformer通过**在较深层中合并图像块**（以灰色显示）来构建分层特征图，并且由于**仅在每个局部窗口（以红色显示）内计算自我关注**，因此对输入图像大小具有**线性计算复杂度**。

  Swin Transformer通过从小尺寸的补丁（灰色轮廓）开始，逐渐合并更深层次的Transformer层中的相邻补丁，构建了一个分层表示。

  线性计算复杂度是通过在分割图像的非重叠窗口内局部计算自我关注来实现的（红色轮廓）。每个窗口中的补丁数量是固定的，因此复杂性与图像大小成线性关系。

- ViT（Vision Transformer）会产生单个低分辨率的特征图，并且由于全局自我关注的计算，对输入图像大小具有二次计算复杂性。

<img src="C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250604164209724.png" alt="image-20250604164209724" style="zoom:67%;" />

Swin Transformer的一个关键设计元素是它在连续的自我关注层之间**移动窗口分区**。移动的窗口桥接了前一层的窗口，在它们之间提供了连接，显著增强了建模能力。

在层l（左）中，采用规则的窗口划分方案，并在每个窗口内计算自关注。在下一层 l+1（右）中，窗口分区被移动，从而产生新的窗口。新窗口中的自我关注计算跨越了层 l 中先前窗口的边界，提供了它们之间的连接。

![image-20250604171836155](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250604171836155.png)

Swin Transformer架构的概述如图所示，其中显示了微型版本（SwinT）。它首先通过**补丁分割模块 Patch Partition**将输入的RGB图像分割成不重叠的补丁（如ViT）。每个补丁都被视为一个“token”，其特征被设置为原始像素RGB值的连接。在我们的实现中，我们使用4×4的补丁大小，因此每个补丁的特征维度为4×4×3=48。在这个原始值特征上应用**线性嵌入(Linear Embedding)**，将其投影到任意维度（表示为C）。在这些补丁令牌上应用了几个具有修改后的自我注意力计算的Transformer块（Swin Transformer块）。Transformer块保持令牌的数量（`H/4×W/4`），与线性嵌入一起称为“第一阶段”。

------

每个 stage 包含若干个 Swin Transformer Block 和一次 downsampling（Patch Merging）：

**▶ Stage 1：**

- 输入维度：`H/4 × W/4 × C`
- 包含两个 Swin Transformer Block（×2）
- 维度保持不变
- 特征分辨率：`H/4 × W/4`，通道数为 `C`

**▶ Patch Merging 1（Stage 1→2）：**

- 将 2×2 邻域 patch 合并，进行下采样
- 分辨率降为 `H/8 × W/8`
- 通道数升为 `2C`

**▶ Stage 2：**

- 输入：`H/8 × W/8 × 2C`
- Swin Transformer Block ×2

**▶ Patch Merging 2（Stage 2→3）：**

- 分辨率变为 `H/16 × W/16`
- 通道数：`4C`

**▶ Stage 3：**

- 输入：`H/16 × W/16 × 4C`
- Swin Transformer Block ×6（最多的一层）

**▶ Patch Merging 3（Stage 3→4）：**

- 分辨率变为 `H/32 × W/32`
- 通道数：`8C`

**▶ Stage 4：**

- 输入：`H/32 × W/32 × 8C`
- Swin Transformer Block ×2

------

**两个连续的 Swin Transformer Block 内部结构**

每个 Swin Transformer Block 包括以下模块：

**✅ 1. Layer Norm（LN）**

标准化操作，用于稳定训练。

**✅ 2. W-MSA（Window-based Multi-head Self Attention）**

- 在 **固定窗口内（如 7×7）** 计算 MSA（self-attention）。
- 避免全局 self-attention 的高计算复杂度（ViT是全局 attention）。
- 时间复杂度从 `O((HW)^2)` 降为 `O((M^2) * (HW / M^2)) = O(HW)`，其中 M 是窗口大小。

**✅ 3. SW-MSA（Shifted Window MSA）**

- 第二个 block 中的窗口相对于前一个 block **偏移一半窗口大小（如 7/2=3）**。
- 实现跨窗口信息交互（否则每个 patch 只能看到固定局部）
- 使用 Masking 和 Padding 处理边界问题。

**✅ 4. MLP（前馈网络）**

- 和普通 Transformer 中一样的结构：两层线性层 + GELU。
- `MLP(x) = Linear(GELU(Linear(x)))`

**🔁 残差连接（Residual）**

- 所有模块都用残差连接

 **第1个Transformer块（W-MSA主导）**

- **输入**：前一层的特征 `z^{l-1}`（来自Stage的初始输入或前序块）
- 核心操作：
  - **LN (LayerNorm)**：对输入特征归一化（图b绿色模块）
  - W-MSA：在常规窗口划分下计算局部注意力（图b红色"W-MSA"）
    - 将特征图分割为 *M*×*M* 非重叠窗口（如 7×7）
    - 仅窗口内做自注意力 → **复杂度从`O(n^2)降至O(n*m^2)`**
  - **残差连接**（`+`符号）：保留原始信息，避免梯度消失
- **输出**：`z^l`将作为下一模块的输入

**第2个Transformer块（SW-MSA主导）**

- **输入**：前一模块的输出 `z^l`
- 关键创新：
  - SW-MSA：使用移位窗口划分（图b蓝色"SW-MSA"）
    - 窗口向右下角偏移 ⌊`M/2`⌋ 像素（图4示意）
    - 例：7×7 窗口 → 偏移后形成**跨越原边界的交互**
  - 效果：
    - 使相邻窗口的像素建立联系
    - **等效全局注意力但计算量不变**
- **残差连接**：维持信息完整流动

------

![image-20250604174600813](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250604174600813.png)

